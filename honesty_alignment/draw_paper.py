import pandas as pd
import numpy as np
import orjson
import os
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
from scipy.optimize import curve_fit
import math
from matplotlib.ticker import FixedLocator
from prepare_label import calculate_consistency_conf_per_n_elements_parallel
import scipy.stats as stats
from scipy.stats import spearmanr

def read_json(path):
    qa_data = []
    with open(path, 'rb') as f:  # äºŒè¿›åˆ¶æ¨¡å¼ï¼Œorjsonéœ€è¦bytes
        for line in f:
            qa_data.append(orjson.loads(line))
    return qa_data


def read_xlsx(file_path):
    # è¯»å– Excel æ–‡ä»¶
    df = pd.read_excel(file_path, header=None)  # é»˜è®¤è¯»å–ç¬¬ä¸€ä¸ªsheet
    prob_score = df.iloc[:, 0]
    consis_score = df.iloc[:, 1]
    greedy_score = df.iloc[:, 2]
    sft_score = df.iloc[:, 3]
    hybrid_score = df.iloc[:, 4]
    return prob_score, consis_score, greedy_score, sft_score, hybrid_score


def get_weighted_avg_scores(domain='in_domain', qa_type='short', base_path='', model_name='Qwen2.5-7B-Instruct', plot_type='auroc'):
    in_domain_datasets = ['nq', 'tq', 'hq', '2wikimultihopqa', 'pararel_patterns']
    ood_datasets = ['complex_web_questions', 'musique', 'web_questions', 'popqa', 'squad']
    used_datasets = in_domain_datasets if domain == 'in_domain' else ood_datasets

    prob_all = []
    consis_all = []
    greedy_all = []
    sft_all = []
    hybrid_all = []
    data_cnt_all = []
    for dataset in used_datasets:
        if model_name == 'Qwen2.5-7B-Instruct' and plot_type == 'auroc':
            file_path = f'{base_path}/{dataset}/{qa_type}_alignment_{plot_type}.xlsx'
            if not os.path.exists(file_path):
                file_path = f'{base_path}/{dataset}/{qa_type}_alignment.xlsx'
        else:
            file_path = f'{base_path}/{dataset}/{qa_type}_alignment_{plot_type}.xlsx'
        prob_score, consis_score, greedy_score, sft_score, hybrid_score = read_xlsx(file_path) # æŒ‰ç…§åˆ—è¯»å–, ä»å·¦åˆ°å³åˆ†åˆ«æ˜¯5ç§åˆ†æ•°, æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªæ ‡æ³¨æ•°æ®é‡
        qa_data = read_json(f'/data/users/nishiyu/res/perception_training/res/{model_name}/{dataset}/test_data/{qa_type}_qa/{dataset}_test_{model_name}_{qa_type}_qa_0.0_0.95_50_sample_1.jsonl')
        data_cnt = len(qa_data)
        data_cnt_all.append(data_cnt)

        prob_all.append(prob_score.values)
        consis_all.append(consis_score.values)
        greedy_all.append(greedy_score.values)
        sft_all.append(sft_score.values)
        hybrid_all.append(hybrid_score.values)

    weights = np.array(data_cnt_all) / sum(data_cnt_all) # æŒ‰æ•°æ®é‡åŠ æƒ
    prob_avg = np.round(np.average(prob_all, axis=0, weights=weights), 2)
    consis_avg = np.round(np.average(consis_all, axis=0, weights=weights), 2)
    greedy_avg = np.round(np.average(greedy_all, axis=0, weights=weights), 2)
    sft_avg = np.round(np.average(sft_all, axis=0, weights=weights), 2)
    hybrid_avg = np.round(np.average(hybrid_all, axis=0, weights=weights), 2)
    print(f'\n==== {qa_type.upper()} QA {domain.upper()} å¹³å‡å¾—åˆ† ====')
    print(f'Prob score avg:   {prob_avg}')
    print(f'Consis score avg: {consis_avg}')
    print(f'Greedy score avg: {greedy_avg}')
    print(f'SFT score avg:    {sft_avg}')
    print(f'Hybrid score avg: {hybrid_avg}')
    return prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg

def get_scores_for_each_dataset(qa_type='short', base_path='', dataset='', model_name='Qwen2.5-7B-Instruct', plot_type='auroc'):
    file_path = f'{base_path}/{dataset}/{qa_type}_alignment_{plot_type}.xlsx'
    prob_score, consis_score, greedy_score, sft_score, hybrid_score = read_xlsx(file_path)

    prob_avg = [round(item,2) for item in prob_score.values]
    consis_avg = [round(item,2) for item in consis_score.values]
    greedy_avg = [round(item,2) for item in greedy_score.values]
    sft_avg = [round(item,2) for item in sft_score.values]
    hybrid_avg = [round(item,2) for item in hybrid_score.values]
    print(f'\n==== {dataset} å¹³å‡å¾—åˆ† ====')
    print(f'Prob score avg:   {prob_avg}')
    print(f'Consis score avg: {consis_avg}')
    print(f'Greedy score avg: {greedy_avg}')
    print(f'SFT score avg:    {sft_avg}')
    print(f'Hybrid score avg: {hybrid_avg}')
    return prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg

def plot_long_qa_baselines():
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    # æ•°æ®
    long_avg = []
    names = ['Prob', 'N-Prob', 'Verbal-0', 'Verbal-10', 'Consis-Lex', 'Consis-Sem']

    for model_name in ['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']:
        file_path = f'/data/users/nishiyu/code/Easy_LLaMA-Factory/perception_training/baseline_res/{model_name}_long_qa_results.xlsx'
        df = pd.read_excel(file_path)
        long_avg.append(df['Back Avg'])

    # è½¬æˆ DataFrame å†å¹³å‡
    long_avg_df = pd.concat(long_avg, axis=1)
    long_avg_mean = long_avg_df.mean(axis=1).round(2).tolist()  # è½¬ list

    # è®¾ç½®å›¾å½¢æ ·å¼
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(14, 8))

    # xè½´ä½ç½®
    n_methods = len(names)
    bar_width = 0.8  # æŸ±å­å®½åº¦åŠ å¤§
    x_positions = range(n_methods)

    # é¢œè‰²
    colors = ['#E74C3C'] * n_methods  # åªç»˜åˆ¶long-QAï¼Œçº¢è‰²

    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    bars = ax.bar(x_positions, long_avg_mean, color=colors, alpha=0.8, 
                  edgecolor='white', linewidth=1.2, width=bar_width)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, long_avg_mean):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{score:.1f}',
                ha='center', va='bottom', fontsize=20, 
                fontweight='bold', color='#2C3E50')

    # è®¾ç½®xè½´æ ‡ç­¾
    ax.set_xticks(list(x_positions))
    ax.set_xticklabels(names, fontsize=22, fontweight='600')

    # è®¾ç½®yè½´æ ‡ç­¾å’ŒèŒƒå›´
    ax.set_ylabel('AUROC', fontsize=22, fontweight='bold', color='#2C3E50')
    ax.set_xlabel('Methods', fontsize=22, fontweight='bold', color='#2C3E50')
    ax.set_ylim(60, 75)

    # ç¾åŒ–ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
    ax.set_axisbelow(True)
    ax.tick_params(axis='y', labelsize=20)

    # è®¾ç½®èƒŒæ™¯è‰²
    fig.patch.set_facecolor('white')
    ax.set_facecolor('#FAFAFA')  # ä¿ç•™èƒŒæ™¯è‰²

    # æ·»åŠ æ ‡é¢˜
    # ax.set_title('Long-QA Baselines Performance', fontsize=24, fontweight='bold', color='#E74C3C', pad=20)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()

    # æ˜¾ç¤ºå›¾åƒ
    plt.show()

    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig('../plot_res/Meta-Llama-3-8B-Instruct/long_qa_baselines_performance.pdf', 
                dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')




def plot_in_domain_and_mmlu(plot_type='align', outdir="comparison", outfile="in_domain_mmlu"):
    """
    ç»˜åˆ¶ 1x4 å›¾ï¼š
    å‰ä¸‰ä¸ªå­å›¾ï¼šä¸‰ä¸ªæ¨¡å‹åœ¨ In-Domain Long-QA ä¸‹çš„æ›²çº¿
    ç¬¬å››ä¸ªå­å›¾ï¼šQwen2.5-7B-Instruct åœ¨ MMLU OOD Long-QA ä¸‹çš„æ›²çº¿
    """
    # ==== Step 1: æ•°æ®æ”¶é›† ====
    actual_train_sizes = [1, 2, 4, 6, 8, 10, 20, 30, 50, 80, 200, 580]
    
    # æ¨¡å‹æ˜ å°„
    model_name_map = {
        'Meta-Llama-3-8B-Instruct': 'Llama3-8B',
        'Qwen2.5-7B-Instruct': 'Qwen2.5-7B',
        'Qwen2.5-14B-Instruct': 'Qwen2.5-14B'
    }
    y_label='Alignment' if plot_type == 'align' else 'AUROC'

    # å­˜æ”¾æ•°æ®
    data_for_all = { "in_domain_long": {}, "mmlu_long": {} }

    # --------- In-domain Long-QA æ•°æ® ---------
    for model_name in ['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']:
        total_sft, total_hybrid = [], []
        for greedy_training_samples in [0]:
            if greedy_training_samples == 0:
                greedy_tail_name = ''
            elif greedy_training_samples <= 10000:
                k = int(greedy_training_samples / 1000)
                greedy_tail_name = f'/_{k}k_training_samples'
            else:
                k = int(greedy_training_samples / 1000)
                greedy_tail_name = f'/_{k}k_training_samples'
            base_path = f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle/{greedy_tail_name}"
            # ä½¿ç”¨å·²æœ‰å‡½æ•°å–æ•°æ®
            prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(
                "in_domain", "long", base_path, model_name, plot_type
            )
            total_sft.append(sft_avg)
            total_hybrid.append(hybrid_avg)
        data_for_all["in_domain_long"][model_name] = {"etc": total_hybrid[0], "dlfc": total_sft[0]}

    # --------- MMLU OOD Long-QA æ•°æ® ---------
    model_name = 'Qwen2.5-7B-Instruct'
    total_sft, total_hybrid = [], []
    for greedy_training_samples in [0]:
        if greedy_training_samples == 0:
            greedy_tail_name = ''
        elif greedy_training_samples <= 10000:
            k = int(greedy_training_samples / 1000)
            greedy_tail_name = f'/_{k}k_training_samples'
        else:
            k = int(greedy_training_samples / 1000)
            greedy_tail_name = f'/_{k}k_training_samples'
        base_path = f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle/{greedy_tail_name}"
        prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_scores_for_each_dataset(
            "long", base_path, "mmlu", model_name, plot_type
        )
        total_sft.append(sft_avg)
        total_hybrid.append(hybrid_avg)
    data_for_all["mmlu_long"][model_name] = {"etc": total_hybrid[0], "dlfc": total_sft[0]}

    # ==== Step 2: ç»˜åˆ¶ ====
    plt.rcParams.update({
        'font.size': 22,
        'font.family': 'sans-serif',
        'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],
        'axes.linewidth': 1.0,
        'axes.labelsize': 23,
        'axes.titlesize': 24,
        'axes.titleweight': 'bold',
        'xtick.labelsize': 21,
        'ytick.labelsize': 21,
        'legend.fontsize': 25,
        'figure.dpi': 300,
        'savefig.dpi': 600,
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.grid': True,
        'grid.alpha': 0.3,
    })

    # é¢œè‰²å’Œæ ·å¼
    etc_style = dict(linestyle='-', marker='o', markersize=6, linewidth=2.2,
                     markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                     color='#2E86AB', markeredgecolor='#2E86AB', zorder=3)
    dlfc_style = dict(linestyle='--', marker='s', markersize=5, linewidth=2.2,
                      markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                      color='#E63946', markeredgecolor='#E63946', zorder=3)

    fig, axes = plt.subplots(1, 4, figsize=(24, 6))
    fig.patch.set_facecolor('#f8f9fa')

    # å‰ä¸‰å¼ ï¼šin-domain long
    for i, model_name in enumerate(['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']):
        ax = axes[i]
        display_name = model_name_map.get(model_name, model_name) + ' (In-Domain)'
        etc_data = data_for_all["in_domain_long"][model_name]["etc"]
        dlfc_data = data_for_all["in_domain_long"][model_name]["dlfc"]
        ax.plot(actual_train_sizes, etc_data, label="ETC", **etc_style)
        ax.plot(actual_train_sizes, dlfc_data, label="DLFC", **dlfc_style)
        ax.set_xscale('log', base=2)
        ax.set_xticks([2**i for i in range(10)])
        ax.set_xticklabels([f'$2^{{{i}}}$' for i in range(10)])
        ax.set_xlim(0.8, 1024)
        if plot_type == 'auroc':
            ax.set_ylim(70, 90)
        else:
            ax.set_ylim(70, 80) # for align
        ax.set_title(display_name, fontweight='bold')
        ax.set_xlabel("Annotation Data Size (k)")
        if i == 0:
            ax.set_ylabel(y_label)
        for spine in ax.spines.values():
            spine.set_visible(False)
        ax.set_facecolor('#f8f9fa')
        ax.grid(True, linestyle='-', alpha=0.6, color='white', linewidth=3, which='major', zorder=1)
        ax.grid(True, linestyle='-', alpha=0.3, color='white', linewidth=1, which='minor', zorder=1)
        
        # å¯ç”¨æ¬¡åˆ»åº¦
        ax.minorticks_on()
        
        # è®¾ç½®åˆ»åº¦çº¿æ ·å¼
        ax.tick_params(axis='x', which='major', 
                        direction='inout', length=8, width=1.2, 
                        color='#666666', labelcolor='#333333')
        ax.tick_params(axis='x', which='minor', 
                        direction='inout', length=4, width=0.8, 
                        color='#999999')
        ax.tick_params(axis='y', which='major', 
                        direction='inout', length=8, width=1.2, 
                        color='#666666', labelcolor='#333333')
        ax.tick_params(axis='y', which='minor', 
                        direction='inout', length=4, width=0.8, 
                        color='#999999')

    # ç¬¬å››å¼ ï¼šMMLU long
    ax = axes[3]
    etc_data = data_for_all["mmlu_long"]['Qwen2.5-7B-Instruct']["etc"]
    dlfc_data = data_for_all["mmlu_long"]['Qwen2.5-7B-Instruct']["dlfc"]
    ax.plot(actual_train_sizes, etc_data, label="ETC", **etc_style)
    ax.plot(actual_train_sizes, dlfc_data, label="DLFC", **dlfc_style)
    ax.set_xscale('log', base=2)
    ax.set_xticks([2**i for i in range(10)])
    ax.set_xticklabels([f'$2^{{{i}}}$' for i in range(10)])
    ax.set_xlim(0.8, 1024)
    if plot_type == 'auroc':
        ax.set_ylim(60, 80)
    else:
        ax.set_ylim(60, 75) # for align
    ax.set_title("Qwen2.5-7B (MMLU)", fontweight='bold')
    ax.set_xlabel("Annotation Data Size (k)")
    for spine in ax.spines.values():
        spine.set_visible(False)
    ax.set_facecolor('#f8f9fa')
    ax.grid(True, linestyle='-', alpha=0.6, color='white', linewidth=3, which='major', zorder=1)
    ax.grid(True, linestyle='-', alpha=0.3, color='white', linewidth=1, which='minor', zorder=1)
    
    # å¯ç”¨æ¬¡åˆ»åº¦
    ax.minorticks_on()
    
    # è®¾ç½®åˆ»åº¦çº¿æ ·å¼
    ax.tick_params(axis='x', which='major', 
                    direction='inout', length=8, width=1.2, 
                    color='#666666', labelcolor='#333333')
    ax.tick_params(axis='x', which='minor', 
                    direction='inout', length=4, width=0.8, 
                    color='#999999')
    ax.tick_params(axis='y', which='major', 
                    direction='inout', length=8, width=1.2, 
                    color='#666666', labelcolor='#333333')
    ax.tick_params(axis='y', which='minor', 
                    direction='inout', length=4, width=0.8, 
                    color='#999999')

    # å›¾ä¾‹
    handles = [
        plt.Line2D([0], [0], **{**etc_style, 'label': 'EliCal'}),
        plt.Line2D([0], [0], **{**dlfc_style, 'label': 'Cal-Only'})
    ]
    fig.legend(handles, ['EliCal', 'Cal-Only'],
               bbox_to_anchor=(0.5, 1.02), loc='center', ncol=2,
               frameon=True, framealpha=0.98, facecolor='#ffffff',
               edgecolor='#dddddd', fontsize=16)

    plt.tight_layout()
    plt.subplots_adjust(top=0.88)

    # ä¿å­˜
    if not os.path.exists(f'../plot_res/{outdir}'):
        os.makedirs(f'../plot_res/{outdir}')
    base_filename = f'../plot_res/{outdir}/{outfile}'
    plt.savefig(f'{base_filename}.pdf', dpi=600, bbox_inches='tight',
                facecolor='#ffffff', edgecolor='none', pad_inches=0.2)
    plt.show()
    print(f"Saved to {base_filename}.pdf")

def plot_ood_only(plot_type='align', outdir="comparison", outfile="ood_only"):
    """
    ç»˜åˆ¶ 1x3 å›¾ï¼š
    ä¸‰ä¸ªå­å›¾ï¼šä¸‰ä¸ªæ¨¡å‹åœ¨ In-Domain Long-QA ä¸‹çš„æ›²çº¿
    """
    # ==== Step 1: æ•°æ®æ”¶é›† ====
    actual_train_sizes = [1, 2, 4, 6, 8, 10, 20, 30, 50, 80, 200, 580]
    
    # æ¨¡å‹æ˜ å°„
    model_name_map = {
        'Meta-Llama-3-8B-Instruct': 'Llama3-8B',
        'Qwen2.5-7B-Instruct': 'Qwen2.5-7B',
        'Qwen2.5-14B-Instruct': 'Qwen2.5-14B'
    }
    y_label='Alignment' if plot_type == 'align' else 'AUROC'

    # å­˜æ”¾æ•°æ®
    data_for_all = { "in_domain_long": {} }

    # --------- In-domain Long-QA æ•°æ® ---------
    for model_name in ['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']:
        total_sft, total_hybrid = [], []
        for greedy_training_samples in [0]:
            if greedy_training_samples == 0:
                greedy_tail_name = ''
            elif greedy_training_samples <= 10000:
                k = int(greedy_training_samples / 1000)
                greedy_tail_name = f'/_{k}k_training_samples'
            else:
                k = int(greedy_training_samples / 1000)
                greedy_tail_name = f'/_{k}k_training_samples'
            base_path = f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle/{greedy_tail_name}"
            prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(
                "ood", "long", base_path, model_name, plot_type
            )
            total_sft.append(sft_avg)
            total_hybrid.append(hybrid_avg)
        data_for_all["in_domain_long"][model_name] = {"etc": total_hybrid[0], "dlfc": total_sft[0]}

    # ==== Step 2: ç»˜åˆ¶ ====
    plt.rcParams.update({
        'font.size': 22,
        'font.family': 'sans-serif',
        'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],
        'axes.linewidth': 1.0,
        'axes.labelsize': 23,
        'axes.titlesize': 24,
        'axes.titleweight': 'bold',
        'xtick.labelsize': 21,
        'ytick.labelsize': 21,
        'legend.fontsize': 25,
        'figure.dpi': 300,
        'savefig.dpi': 600,
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.grid': True,
        'grid.alpha': 0.3,
    })

    # é¢œè‰²å’Œæ ·å¼
    etc_style = dict(linestyle='-', marker='o', markersize=6, linewidth=2.2,
                     markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                     color='#2E86AB', markeredgecolor='#2E86AB', zorder=3)
    dlfc_style = dict(linestyle='--', marker='s', markersize=5, linewidth=2.2,
                      markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                      color='#E63946', markeredgecolor='#E63946', zorder=3)

    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # æ”¹æˆ1x3
    fig.patch.set_facecolor('#f8f9fa')

    # ä¸‰å¼  in-domain å­å›¾
    for i, model_name in enumerate(['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']):
        ax = axes[i]
        display_name = model_name_map.get(model_name, model_name) + ' (OOD)'
        etc_data = data_for_all["in_domain_long"][model_name]["etc"]
        dlfc_data = data_for_all["in_domain_long"][model_name]["dlfc"]
        ax.plot(actual_train_sizes, etc_data, label="ETC", **etc_style)
        ax.plot(actual_train_sizes, dlfc_data, label="DLFC", **dlfc_style)
        ax.set_xscale('log', base=2)
        ax.set_xticks([2**i for i in range(10)])
        ax.set_xticklabels([f'$2^{{{i}}}$' for i in range(10)])
        ax.set_xlim(0.8, 1024)
        # if plot_type == 'auroc':
        ax.set_ylim(70, 90)
        # else:
        # ax.set_ylim(70, 80)  # for align
        ax.set_title(display_name, fontweight='bold')
        ax.set_xlabel("Annotation Data Size (k)")
        if i == 0:
            ax.set_ylabel(y_label)
        for spine in ax.spines.values():
            spine.set_visible(False)
        ax.set_facecolor('#f8f9fa')
        ax.grid(True, linestyle='-', alpha=0.6, color='white', linewidth=3, which='major', zorder=1)
        ax.grid(True, linestyle='-', alpha=0.3, color='white', linewidth=1, which='minor', zorder=1)
        ax.minorticks_on()
        ax.tick_params(axis='x', which='major', direction='inout', length=8, width=1.2, color='#666666', labelcolor='#333333')
        ax.tick_params(axis='x', which='minor', direction='inout', length=4, width=0.8, color='#999999')
        ax.tick_params(axis='y', which='major', direction='inout', length=8, width=1.2, color='#666666', labelcolor='#333333')
        ax.tick_params(axis='y', which='minor', direction='inout', length=4, width=0.8, color='#999999')

    # å›¾ä¾‹
    handles = [
        plt.Line2D([0], [0], **{**etc_style, 'label': 'EliCal'}),
        plt.Line2D([0], [0], **{**dlfc_style, 'label': 'Cal-Only'})
    ]
    fig.legend(handles, ['EliCal', 'Cal-Only'],
               bbox_to_anchor=(0.5, 1.02), loc='center', ncol=2,
               frameon=True, framealpha=0.98, facecolor='#ffffff',
               edgecolor='#dddddd', fontsize=16)

    plt.tight_layout()
    plt.subplots_adjust(top=0.88)

    # ä¿å­˜
    if not os.path.exists(f'../plot_res/{outdir}'):
        os.makedirs(f'../plot_res/{outdir}')
    base_filename = f'../plot_res/{outdir}/{outfile}'
    plt.savefig(f'{base_filename}.pdf', dpi=600, bbox_inches='tight',
                facecolor='#ffffff', edgecolor='none', pad_inches=0.2)
    plt.show()
    print(f"Saved to {base_filename}.pdf")


def plot_effects_of_training_size_for_elicitation():
    def plot_for_all_dataset_across_greedy_training_samples(qa_type, dataset, prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg, outdir):
        """
        ç»˜åˆ¶elicitationæ•ˆæœéšé¢„è®­ç»ƒæ•°æ®é‡çš„å˜åŒ–æ›²çº¿ï¼Œç²¾ç¾ç‰ˆæœ¬
        æ¨ªè½´ä½¿ç”¨çœŸå®æ•°å€¼ï¼ˆçº¿æ€§å°ºåº¦ï¼‰ï¼Œæ ‡æ³¨æ¯ä¸ªç‚¹ç›¸å¯¹äºconsis_avg[0][0]çš„ç™¾åˆ†æ¯”
        """
        import matplotlib.pyplot as plt
        import numpy as np
        from scipy.optimize import curve_fit
        from scipy.interpolate import interp1d
        import math
        import os
        
        def logarithmic_func(x, a, b, c):
            return a * np.log(x + b) + c

        pretrain_data_k = [10, 20, 30, 50, 80, 200, 580]  
        pretrain_labels = ['10K', '20K', '30K', '50K', '80K', '200K', '580K']

        # ---------- ç²¾ç¾é£æ ¼è®¾ç½® ----------
        plt.style.use('default')  # é‡ç½®æ ·å¼
        plt.rcParams.update({
            'font.family': ['Times New Roman', 'DejaVu Serif', 'serif'],
            'font.size': 20,
            'axes.linewidth': 1.5,
            'axes.spines.top': False,
            'axes.spines.right': False,
            'axes.grid': True,
            'grid.alpha': 0.3,
            'grid.linewidth': 0.8,
            'figure.facecolor': 'white',
            'axes.facecolor': '#fafafa',
            'text.color': '#2C3E50',
            'axes.labelcolor': '#2C3E50',
            'xtick.color': '#2C3E50',
            'ytick.color': '#2C3E50'
        })

        # é¢œè‰²æ–¹æ¡ˆ
        primary_color = '#6C5CE7'  # ä¼˜é›…ç´«è‰²
        secondary_color = '#74B9FF'  # æŸ”å’Œè“è‰²
        accent_color = '#FD79A8'  # æ¸©æš–ç²‰è‰²
        text_color = '#2C3E50'  # æ·±ç°è“

        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))

        # ============ æ•°æ®å¤„ç† ============
        greedy_values = [greedy_avg[i][0] for i in range(len(pretrain_labels))]
        baseline_value = consis_avg[0][0]  # åŸºå‡†å€¼
        percentages = [(val / baseline_value) * 100 for val in greedy_values]
        # ============ ç»˜åˆ¶åŸºå‡†çº¿ ============
        # ============ ç»˜åˆ¶åŸºå‡†çº¿ ============
        baseline_value = consis_avg[0][0]
        ax.axhline(y=baseline_value, color=secondary_color, linestyle='--', linewidth=2,
                alpha=0.8, zorder=0, label='Consis-Sem')
        
        # æ·»åŠ åŸºå‡†çº¿æ ‡ç­¾
        ax.text(max(pretrain_data_k) * 0.98, baseline_value * 1.01, 
                f'Consis-Sem: {baseline_value:.3f}',
                color=secondary_color, fontsize=20, fontweight='bold',
                va='bottom', ha='right', alpha=0.8)

        # ============ ç»˜åˆ¶å¹³æ»‘æ›²çº¿ ============
        x_smooth = np.linspace(min(pretrain_data_k), max(pretrain_data_k), 500)
        
        try:
            popt, _ = curve_fit(logarithmic_func, pretrain_data_k, greedy_values, maxfev=20000)
            y_smooth = logarithmic_func(x_smooth, *popt)
            # ç»˜åˆ¶æ¸å˜æ•ˆæœçš„æ›²çº¿
            ax.plot(x_smooth, y_smooth, color=primary_color, linewidth=4, 
                    alpha=0.8, zorder=2, label='Elicitation Performance')
            # æ·»åŠ é˜´å½±æ•ˆæœ
            ax.fill_between(x_smooth, y_smooth, alpha=0.15, color=primary_color, zorder=1)
        except Exception:
            f_interp = interp1d(pretrain_data_k, greedy_values, kind='cubic', fill_value='extrapolate')
            y_smooth = f_interp(x_smooth)
            ax.plot(x_smooth, y_smooth, color=primary_color, linewidth=4, 
                    alpha=0.8, zorder=2, label='Elicitation Performance')
            ax.fill_between(x_smooth, y_smooth, alpha=0.15, color=primary_color, zorder=1)

        # ============ ç»˜åˆ¶æ•°æ®ç‚¹ ============
        scatter = ax.scatter(pretrain_data_k, greedy_values, 
                            color=accent_color, s=200, zorder=5,
                            marker='o', edgecolors='white', linewidth=3, 
                            alpha=0.95, label='Data Points')

        # æ·»åŠ å†…åœˆ
        ax.scatter(pretrain_data_k, greedy_values, 
                color=primary_color, s=80, zorder=6,
                marker='o', alpha=0.8)

    # ============ æ·»åŠ ç™¾åˆ†æ¯”æ ‡æ³¨ ============
        placed_positions = []  # å­˜æ”¾å·²æœ‰æ ‡æ³¨ä½ç½®ï¼Œé¿å…é‡å 
        x_offset_factor = 0.15  # æ§åˆ¶æ–‡æœ¬ç›¸å¯¹æ¨ªåæ ‡çš„åç§»æ¯”ä¾‹ï¼ˆå¯è°ƒï¼‰

        min_sep = 0.05 * (max(greedy_values) - min(greedy_values))  # æœ€å°å‚ç›´é—´éš”

        for i, (x, y, pct) in enumerate(zip(pretrain_data_k, greedy_values, percentages)):
            # åˆå§‹ä½ç½®ï¼šå…ˆæ”¾åœ¨ç‚¹ä¸Šæ–¹
            y_pos = y + min_sep
            x_offset = x_offset_factor * x  

            # é¿å…å’Œå·²æœ‰æ ‡æ³¨é‡å 
            while any(abs(y_pos - yy) < min_sep for yy in placed_positions):
                y_pos += min_sep  

            placed_positions.append(y_pos)

            # ax.annotate(f'{pct:.1f}%', 
            #         xy=(x, y), xytext=(x + x_offset, y_pos),
            #         ha='center', va='bottom',
            #         fontsize=16, fontweight='bold',
            #         rotation=-30,  # ğŸ”¥ æ—‹è½¬ 30 åº¦
            #         rotation_mode='anchor',  # ä¿è¯æ—‹è½¬æ—¶é”šç‚¹å¯¹é½
            #         color=text_color,
            #         bbox=dict(boxstyle='round,pad=0.3',
            #                   facecolor='white',
            #                   edgecolor=primary_color,
            #                   alpha=0.9),
            #         arrowprops=dict(arrowstyle='->',
            #                         color=primary_color,
            #                         alpha=0.7,
            #                         lw=1.5),
            #         zorder=7)


        # ============ åæ ‡è½´è®¾ç½® ============
        # æ™ºèƒ½åˆ»åº¦
        max_pow = int(math.ceil(math.log2(max(pretrain_data_k))))
        min_pow = int(math.floor(math.log2(min(pretrain_data_k))))
        candidate_ticks = [2**i for i in range(min_pow, max_pow + 1)]
        
        if len(candidate_ticks) > 6:
            step = math.ceil(len(candidate_ticks)/6)
            ticks = candidate_ticks[::step]
        else:
            ticks = candidate_ticks
        
        tick_labels = [f'$2^{{{int(np.log2(t))}}}$' for t in ticks]

        ax.set_xticks(ticks)
        ax.set_xticklabels(tick_labels, fontsize=20, color=text_color)
        
        # è®¾ç½®åæ ‡è½´èŒƒå›´
        x_pad = (max(pretrain_data_k) - min(pretrain_data_k)) * 0.08
        y_pad = (max(greedy_values) - min(greedy_values)) * 0.1
        ax.set_xlim(min(pretrain_data_k)-x_pad, max(pretrain_data_k)+x_pad)
        ax.set_ylim(min(greedy_values)-y_pad, 75)

        # ============ æ ‡ç­¾å’Œæ ‡é¢˜ ============
        ax.set_xlabel('Data Size for Confidence Elicitation (k)', 
                    fontsize=20, fontweight='bold', color=text_color)
        ax.set_ylabel('AUROC', 
                    fontsize=20, fontweight='bold', color=text_color)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)


        # ============ ç½‘æ ¼ä¼˜åŒ– ============
        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8)
        ax.set_axisbelow(True)

        # ============ ä¿å­˜å›¾ç‰‡ ============
        plt.tight_layout(pad=2.0)
        
        if not os.path.exists(outdir):
            os.makedirs(outdir)
        filename = f'{qa_type}_{dataset}_elicitation_scaling_beautiful.pdf'
        filepath = os.path.join(outdir, filename)
        plt.savefig(filepath, dpi=400, bbox_inches='tight', 
                    facecolor='white', edgecolor='none')
        plt.show()

        print(f"ç²¾ç¾å›¾ç‰‡å·²ä¿å­˜åˆ°: {filepath}")
        print(f"åŸºå‡†å€¼ (consis_avg[0][0]): {baseline_value:.4f}")
        print("å„æ•°æ®ç‚¹ç›¸å¯¹åŸºå‡†å€¼çš„ç™¾åˆ†æ¯”:")
        for label, pct in zip(pretrain_labels, percentages):
            print(f"  {label}: {pct:.1f}%")

    plot_type = 'auroc'
    data_for_all_settings = {}
    for domain in ['in_domain']:
        for qa_type in ['long']:
            data_across_models = {}
            for model_name in ['Qwen2.5-7B-Instruct']:
                print(f'{domain}-{qa_type}-{model_name}')
                total_prob = []
                total_consis = []
                total_greedy = []
                total_sft = []
                total_hybrid = []
                for greedy_training_samples in [10000, 20000, 30000, 50000, 80000, 200000, 0]:
                    if greedy_training_samples == 0:
                        greedy_epochs=10
                        greedy_tail_name = ''
                    elif greedy_training_samples <= 10000:
                        greedy_epochs=50
                        k=int(greedy_training_samples/1000)
                        greedy_tail_name=f'/_{k}k_training_samples'
                    else:
                        greedy_epochs=15
                        k=int(greedy_training_samples/1000)
                        greedy_tail_name=f'/_{k}k_training_samples'
                    base_path = f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle/{greedy_tail_name}"
                    outdir = f'{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle_greedy_training_samples/{greedy_training_samples}'
                    # ä¸€ç§é¢„è®­ç»ƒæ•°æ®é‡ä¸‹, ä¸åŒæ–¹æ³•éšæ ‡æ³¨æ•°æ®é‡å˜åŒ–çš„AUROCåˆ†æ•°
                    prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(domain, qa_type, base_path, model_name, plot_type)
                    # prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_scores_for_each_dataset(qa_type, base_path, 'mmlu', model_name)
                    total_prob.append(prob_avg)
                    total_consis.append(consis_avg)
                    total_greedy.append(greedy_avg)
                    total_sft.append(sft_avg)
                    total_hybrid.append(hybrid_avg)
                    data_across_models[model_name] = {'etc': total_hybrid, 'dlfc': total_sft}
                plot_for_all_dataset_across_greedy_training_samples(qa_type, domain, total_prob, total_consis, total_greedy, total_sft, total_hybrid, outdir)


def plot_in_and_ood_mlp(plot_type='auroc', outdir="comparison", outfile="in_and_ood"):
    """
    ç»˜åˆ¶ 2x3 å›¾ï¼š
    ç¬¬ä¸€è¡Œï¼šä¸‰ä¸ªæ¨¡å‹åœ¨ In-Domain Long-QA ä¸‹çš„æ›²çº¿
    ç¬¬äºŒè¡Œï¼šä¸‰ä¸ªæ¨¡å‹åœ¨ OOD Long-QA ä¸‹çš„æ›²çº¿
    æ¯åˆ—ä»£è¡¨ä¸€ä¸ªæ¨¡å‹
    """
    import os
    import matplotlib.pyplot as plt

    # ==== Step 1: æ•°æ®æ”¶é›† ====
    actual_train_sizes = [1, 2, 4, 6, 8, 10, 20, 30, 50, 80, 200, 580]
    
    model_name_map = {
        'Meta-Llama-3-8B-Instruct': 'Llama3-8B',
        'Qwen2.5-7B-Instruct': 'Qwen2.5-7B',
        'Qwen2.5-14B-Instruct': 'Qwen2.5-14B'
    }
    y_label = 'Alignment' if plot_type == 'align' else 'AUROC'

    data_for_all = { "in_domain_long": {}, "ood_long": {} }

    # --------- In-domain & OOD Long-QA æ•°æ® ---------
    for model_name in ['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']:
        # In-domain
        prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(
            "in_domain", "long",
            f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle/mlp",
            model_name, plot_type
        )
        data_for_all["in_domain_long"][model_name] = {"etc": hybrid_avg, "dlfc": sft_avg}

        # OOD
        prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(
            "ood", "long",
            f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle/mlp",
            model_name, plot_type
        )
        data_for_all["ood_long"][model_name] = {"etc": hybrid_avg, "dlfc": sft_avg}

    # ==== Step 2: ç»˜åˆ¶ ====
    plt.rcParams.update({
        'font.size': 22,
        'font.family': 'sans-serif',
        'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],
        'axes.linewidth': 1.0,
        'axes.labelsize': 23,
        'axes.titlesize': 24,
        'axes.titleweight': 'bold',
        'xtick.labelsize': 21,
        'ytick.labelsize': 21,
        'legend.fontsize': 25,
        'figure.dpi': 300,
        'savefig.dpi': 600,
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.grid': True,
        'grid.alpha': 0.3,
    })

    etc_style = dict(linestyle='-', marker='o', markersize=6, linewidth=2.2,
                     markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                     color='#2E86AB', markeredgecolor='#2E86AB', zorder=3)
    dlfc_style = dict(linestyle='--', marker='s', markersize=5, linewidth=2.2,
                      markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                      color='#E63946', markeredgecolor='#E63946', zorder=3)

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # 2x3
    fig.patch.set_facecolor('#f8f9fa')

    for row, domain in enumerate(["in_domain_long", "ood_long"]):
        for col, model_name in enumerate(['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']):
            ax = axes[row, col]
            display_name = model_name_map.get(model_name, model_name)
            display_name += " (In-Domain)" if row == 0 else " (OOD)"

            etc_data = data_for_all[domain][model_name]["etc"]
            dlfc_data = data_for_all[domain][model_name]["dlfc"]
            ax.plot(actual_train_sizes, etc_data, label="ETC", **etc_style)
            ax.plot(actual_train_sizes, dlfc_data, label="DLFC", **dlfc_style)
            
            ax.set_xscale('log', base=2)
            ax.set_xticks([2**i for i in range(10)])
            ax.set_xticklabels([f'$2^{{{i}}}$' for i in range(10)])
            ax.set_xlim(0.8, 1024)
            ax.set_ylim(60, 85)
            ax.set_title(display_name, fontweight='bold')
            ax.set_xlabel("Annotation Data Size (k)")
            if col == 0:
                ax.set_ylabel(y_label)

            for spine in ax.spines.values():
                spine.set_visible(False)
            ax.set_facecolor('#f8f9fa')
            ax.grid(True, linestyle='-', alpha=0.6, color='white', linewidth=3, which='major', zorder=1)
            ax.grid(True, linestyle='-', alpha=0.3, color='white', linewidth=1, which='minor', zorder=1)
            ax.minorticks_on()
            ax.tick_params(axis='x', which='major', direction='inout', length=8, width=1.2, color='#666666', labelcolor='#333333')
            ax.tick_params(axis='x', which='minor', direction='inout', length=4, width=0.8, color='#999999')
            ax.tick_params(axis='y', which='major', direction='inout', length=8, width=1.2, color='#666666', labelcolor='#333333')
            ax.tick_params(axis='y', which='minor', direction='inout', length=4, width=0.8, color='#999999')

    handles = [
        plt.Line2D([0], [0], **{**etc_style, 'label': 'EliCal'}),
        plt.Line2D([0], [0], **{**dlfc_style, 'label': 'Cali-Only'})
    ]
    fig.legend(handles, ['EliCal', 'Cal-Only'],
               bbox_to_anchor=(0.5, 1.02), loc='center', ncol=2,
               frameon=True, framealpha=0.98, facecolor='#ffffff',
               edgecolor='#dddddd', fontsize=16)

    plt.tight_layout()
    plt.subplots_adjust(top=0.92)

    if not os.path.exists(f'../plot_res/{outdir}'):
        os.makedirs(f'../plot_res/{outdir}')
    base_filename = f'../plot_res/{outdir}/{outfile}'
    plt.savefig(f'{base_filename}.pdf', dpi=600, bbox_inches='tight',
                facecolor='#ffffff', edgecolor='none', pad_inches=0.2)
    plt.show()
    print(f"Saved to {base_filename}.pdf")

def plot_in_and_ood_ece(plot_type='ece', outdir="comparison", outfile="in_and_ood"):
    """
    ç»˜åˆ¶ 2x3 å›¾ï¼š
    ç¬¬ä¸€è¡Œï¼šä¸‰ä¸ªæ¨¡å‹åœ¨ In-Domain Long-QA ä¸‹çš„æ›²çº¿
    ç¬¬äºŒè¡Œï¼šä¸‰ä¸ªæ¨¡å‹åœ¨ OOD Long-QA ä¸‹çš„æ›²çº¿
    æ¯åˆ—ä»£è¡¨ä¸€ä¸ªæ¨¡å‹
    """
    import os
    import matplotlib.pyplot as plt

    # ==== Step 1: æ•°æ®æ”¶é›† ====
    actual_train_sizes = [1, 2, 4, 6, 8, 10, 20, 30, 50, 80, 200, 580]
    
    model_name_map = {
        'Meta-Llama-3-8B-Instruct': 'Llama3-8B',
        'Qwen2.5-7B-Instruct': 'Qwen2.5-7B',
        'Qwen2.5-14B-Instruct': 'Qwen2.5-14B'
    }
    y_label = 'ECE'

    data_for_all = { "in_domain_long": {}, "ood_long": {} }

    # --------- In-domain & OOD Long-QA æ•°æ® ---------
    for model_name in ['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']:
        # In-domain
        prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(
            "in_domain", "long",
            f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle",
            model_name, plot_type
        )
        data_for_all["in_domain_long"][model_name] = {"etc": hybrid_avg, "dlfc": sft_avg}

        # OOD
        prob_avg, consis_avg, greedy_avg, sft_avg, hybrid_avg = get_weighted_avg_scores(
            "ood", "long",
            f"./res/{model_name}/pararel_patterns-nq-tq-hq-2wikimultihopqa_no_shuffle",
            model_name, plot_type
        )
        data_for_all["ood_long"][model_name] = {"etc": hybrid_avg, "dlfc": sft_avg}

    # ==== Step 2: ç»˜åˆ¶ ====
    plt.rcParams.update({
        'font.size': 22,
        'font.family': 'sans-serif',
        'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],
        'axes.linewidth': 1.0,
        'axes.labelsize': 23,
        'axes.titlesize': 24,
        'axes.titleweight': 'bold',
        'xtick.labelsize': 21,
        'ytick.labelsize': 21,
        'legend.fontsize': 25,
        'figure.dpi': 300,
        'savefig.dpi': 600,
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.grid': True,
        'grid.alpha': 0.3,
    })

    etc_style = dict(linestyle='-', marker='o', markersize=6, linewidth=2.2,
                     markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                     color='#2E86AB', markeredgecolor='#2E86AB', zorder=3)
    dlfc_style = dict(linestyle='--', marker='s', markersize=5, linewidth=2.2,
                      markerfacecolor='white', markeredgewidth=2, alpha=0.9,
                      color='#E63946', markeredgecolor='#E63946', zorder=3)

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # 2x3
    fig.patch.set_facecolor('#f8f9fa')

    for row, domain in enumerate(["in_domain_long", "ood_long"]):
        for col, model_name in enumerate(['Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'Meta-Llama-3-8B-Instruct']):
            ax = axes[row, col]
            display_name = model_name_map.get(model_name, model_name)
            display_name += " (In-Domain)" if row == 0 else " (OOD)"

            etc_data = data_for_all[domain][model_name]["etc"]
            dlfc_data = data_for_all[domain][model_name]["dlfc"]
            ax.plot(actual_train_sizes, etc_data, label="ETC", **etc_style)
            ax.plot(actual_train_sizes, dlfc_data, label="DLFC", **dlfc_style)
            
            ax.set_xscale('log', base=2)
            ax.set_xticks([2**i for i in range(10)])
            ax.set_xticklabels([f'$2^{{{i}}}$' for i in range(10)])
            ax.set_xlim(0.8, 1024)
            ax.set_ylim(0, 0.3)
            ax.set_title(display_name, fontweight='bold')
            ax.set_xlabel("Annotation Data Size (k)")
            if col == 0:
                ax.set_ylabel(y_label)

            for spine in ax.spines.values():
                spine.set_visible(False)
            ax.set_facecolor('#f8f9fa')
            ax.grid(True, linestyle='-', alpha=0.6, color='white', linewidth=3, which='major', zorder=1)
            ax.grid(True, linestyle='-', alpha=0.3, color='white', linewidth=1, which='minor', zorder=1)
            ax.minorticks_on()
            ax.tick_params(axis='x', which='major', direction='inout', length=8, width=1.2, color='#666666', labelcolor='#333333')
            ax.tick_params(axis='x', which='minor', direction='inout', length=4, width=0.8, color='#999999')
            ax.tick_params(axis='y', which='major', direction='inout', length=8, width=1.2, color='#666666', labelcolor='#333333')
            ax.tick_params(axis='y', which='minor', direction='inout', length=4, width=0.8, color='#999999')

    handles = [
        plt.Line2D([0], [0], **{**etc_style, 'label': 'EliCal'}),
        plt.Line2D([0], [0], **{**dlfc_style, 'label': 'Cal-Only'})
    ]
    fig.legend(handles, ['EliCal', 'Cal-Only'],
               bbox_to_anchor=(0.5, 1.02), loc='center', ncol=2,
               frameon=True, framealpha=0.98, facecolor='#ffffff',
               edgecolor='#dddddd', fontsize=16)

    plt.tight_layout()
    plt.subplots_adjust(top=0.92)

    if not os.path.exists(f'../plot_res/{outdir}'):
        os.makedirs(f'../plot_res/{outdir}')
    base_filename = f'../plot_res/{outdir}/{outfile}'
    plt.savefig(f'{base_filename}.pdf', dpi=600, bbox_inches='tight',
                facecolor='#ffffff', edgecolor='none', pad_inches=0.2)
    plt.show()
    print(f"Saved to {base_filename}.pdf")

def plot_confidence_values(confidence1, confidence2, model_name, dataset, qa_type, n_bins=1000):
    """
    Plot confidence comparison (line + error bars) and confidence histogram (sample counts) side by side.
    """
    import matplotlib.pyplot as plt
    import numpy as np
    from scipy.stats import spearmanr

    conf1 = np.array(confidence1)
    conf2 = np.array(confidence2)
    spearman_corr, p_value = spearmanr(conf1, conf2)
    
    # Colors
    colors = {
        'conf1': '#0F4C75',      
        'conf2': '#BB2649',      
        'conf1_light': '#3282B8',
        'conf2_light': '#E84A5F',
        'grid': '#E9ECEF',       
        'background': '#FDFDFD'
    }

    # --- Step 1: Prepare line plot (confidence comparison) ---
    idx_sorted = np.argsort(conf1)
    bin_size = len(conf1) // n_bins
    conf1_binned, conf2_binned, conf1_se, conf2_se = [], [], [], []

    for i in range(n_bins):
        start = i * bin_size
        end = start + bin_size if i < n_bins-1 else len(conf1)
        bin1 = conf1[idx_sorted[start:end]]
        bin2 = conf2[idx_sorted[start:end]]
        conf1_binned.append(np.mean(bin1))
        conf2_binned.append(np.mean(bin2))
        conf1_se.append(np.std(bin1)/np.sqrt(len(bin1)))
        conf2_se.append(np.std(bin2)/np.sqrt(len(bin2)))

    x_bins = conf1_binned

    # --- Step 2: Prepare histogram (sample counts) ---
    hist_bins = np.linspace(0, 1, 11)  # 10 bins: 0.0-0.1, 0.1-0.2, ..., 0.9-1.0
    counts1, _ = np.histogram(conf1, bins=hist_bins)
    counts2, _ = np.histogram(conf2, bins=hist_bins)
    bin_centers = (hist_bins[:-1] + hist_bins[1:]) / 2

    # --- Step 3: Create 1x2 subplot ---
    fig, axes = plt.subplots(1, 2, figsize=(14,6))
    axes[0].set_facecolor(colors['background'])
    axes[1].set_facecolor(colors['background'])

    # --- Left: confidence comparison (sampled with error band between lines) ---
    idx_sorted = np.argsort(conf1)
    n_samples = len(conf1)
    n_bins = 20  # æ§åˆ¶ç‚¹æ•°é‡
    bin_edges = np.linspace(0, n_samples, n_bins+1, dtype=int)

    x_vals, conf1_sampled, conf2_sampled = [], [], []

    for start, end in zip(bin_edges[:-1], bin_edges[1:]):
        bin1 = conf1[idx_sorted[start:end]]
        bin2 = conf2[idx_sorted[start:end]]
        x_vals.append((start + end) / 2)
        conf1_sampled.append(np.mean(bin1))
        conf2_sampled.append(np.mean(bin2))

    # ç»˜åˆ¶çº¿æ¡
    axes[0].plot(x_vals, conf1_sampled, 'o-', label='Accuracy',
                linewidth=4, markersize=10, color=colors['conf1'],
                markerfacecolor=colors['conf1_light'], markeredgecolor=colors['conf1'])
    axes[0].plot(x_vals, conf2_sampled, 's-', label='Confidence',
                linewidth=4, markersize=10, color=colors['conf2'],
                markerfacecolor=colors['conf2_light'], markeredgecolor=colors['conf2'])

    # ç»˜åˆ¶ä¸¤æ¡çº¿ä¹‹é—´çš„æµ…é˜´å½±
    axes[0].fill_between(x_vals, conf1_sampled, conf2_sampled, color='gray', alpha=0.2)

    axes[0].set_xlabel('Questions Sorted by Accuracy', fontsize=20, fontweight='600')
    axes[0].set_ylabel('Value', fontsize=20, fontweight='600')
    axes[0].set_title(f'Self-Consistency Confidence vs. Accuracy', fontsize=20, fontweight='bold', pad=20)
    axes[0].grid(True, alpha=0.3, color=colors['grid'], linestyle='-', linewidth=0.8)
    axes[0].legend(loc='upper left', frameon=True, fancybox=True, shadow=True,
                framealpha=0.95, fontsize=20, facecolor='#FFFFFF', edgecolor='#BDC3C7')

    # Spearman correlation
    spearman_corr, p_value = spearmanr(conf1, conf2)
    significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
    axes[0].text(0.98, 0.02, f'Spearman Ï = {spearman_corr:.3f}', 
                transform=axes[0].transAxes, fontsize=20,
                verticalalignment='bottom', horizontalalignment='right',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor='#BDC3C7'))
    axes[0].tick_params(axis='both', labelsize=20)



    # --- Right: histogram (sample counts as percentage) ---
    total_samples = len(conf1)
    counts1_pct = counts1 / total_samples * 100
    counts2_pct = counts2 / total_samples * 100

    width = 0.04  # bar width
    # --- Right: histogram (sample counts as percentage) ---
    axes[1].bar(bin_centers - width/2, counts1_pct, width=width, label='Accuracy', color=colors['conf1_light'])
    axes[1].bar(bin_centers + width/2, counts2_pct, width=width, label='Confidence', color=colors['conf2_light'])

    # è®¾ç½®å›ºå®šåˆ»åº¦ 0.0, 0.1, ..., 1.0
    x_ticks = np.arange(0, 1.01, 0.1)
    axes[1].set_xticks(x_ticks)
    axes[1].set_xticklabels([f"{x:.1f}" for x in x_ticks], fontsize=20)

    axes[1].set_xlabel('Confidence/Accuracy', fontsize=20, fontweight='600')
    axes[1].set_ylabel('Percentage (%)', fontsize=20, fontweight='600')
    axes[1].set_title('Confidence/Accuracy Distribution', fontsize=20, fontweight='bold', pad=20)
    axes[1].grid(True, alpha=0.3, color=colors['grid'], linestyle='-', linewidth=0.8)
    axes[1].legend(frameon=True, fancybox=True, shadow=True, framealpha=0.95, fontsize=20, facecolor='#FFFFFF', edgecolor='#BDC3C7')

    # åæ ‡è½´åˆ»åº¦å­—ä½“ç»Ÿä¸€æ”¹ä¸º20
    axes[1].tick_params(axis='y', labelsize=20)

    plt.tight_layout()
    plt.savefig(f'./paper_detail_res/{dataset}_{model_name}_{qa_type}_confidence_line_hist.pdf', 
                dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
    plt.show()



def correlation_between_consis_and_acc(model_name='Qwen2.5-7B-Instruct', dataset='tq', qa_type='long_qa'):
    base_dir = '/data/users/nishiyu/code/Easy_LLaMA-Factory/perception_training/baseline_res/record_per_dataset'
    file_path = f'{base_dir}/{model_name}/{qa_type}/{dataset}.jsonl'
    data = read_json(file_path)
    print(f'dataset: {dataset}')
    sorry = []
    sample_acc = []
    acc = []
    sem_conf = []
    
    for item in data:
        sample_acc.append(item['sample_acc'])
        acc.append(item['acc'])
        sem_conf.append(item['consis_sem_conf'])

    corr, p_value = spearmanr(sem_conf, sample_acc)

    print("Spearman correlation:", corr)
    print("p-value:", p_value)
    plot_confidence_values(sample_acc, sem_conf, model_name, dataset, qa_type, 20)
    # plot_ranking_positions(sample_acc, sem_conf, acc, model_name, dataset, qa_type)


if __name__ == '__main__':
    # baselines
    # plot_long_qa_baselines()

    # in-domain and mmlu
    # plot_in_domain_and_mmlu('align')

    # ood
    plot_ood_only('align')

    # plot_in_and_ood_mlp()

    # plot_in_and_ood_ece()

    # correlation_between_consis_and_acc()
