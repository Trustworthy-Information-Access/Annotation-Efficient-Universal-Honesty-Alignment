import torch
from torch import nn
# from torch.utils.data import Dataset
from datasets import Dataset
from transformers import AutoTokenizer, AutoModel, TrainingArguments
from transformers.modeling_outputs import CausalLMOutput
from trl import SFTTrainer
from peft import get_peft_model, LoraConfig, PeftModel
import argparse
import os
from prepare_label import construct_right_answer_conf, construct_greedy_answer_conf
import wandb
import random
import numpy as np
import json
from accelerate import Accelerator
from accelerate.utils import set_seed as accelerate_set_seed
import torch.distributed as dist

### 2. Data Collator: tokenize text + attach soft label
# é»˜è®¤çš„collatoræ˜¯æŒ‰ç…§language modelingçš„æ–¹å¼æ„é€ labels, labels=input_idsã€‚ç„¶åå¯¹ä¸€ä¸ªbatchçš„æ•°æ®åšpadding
class DataCollatorWithVectorLabel:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, batch):
        # è¿›æ¥çš„batchæ˜¯æŒ‰ç…§trainerå†…éƒ¨é€»è¾‘åˆ†è¯åçš„, æ¯ä¸ªåˆ†è¯æœ€åéƒ½åŠ äº†ä¸€ä¸ªeos_token
        # è¿™é‡ŒæŒ‰è‡ªå·±çš„é€»è¾‘é‡æ–°å¤„ç†ä¸€ä¸‹
        texts = [item["text"] for item in batch]
        labels = [item["label"] for item in batch]

        tokenized = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )

        tokenized["labels"] = torch.tensor(labels, dtype=torch.float32)
        return tokenized


### 3. Model with classification head (output 10-dim vector)
class LMWithVectorHead(nn.Module):
    def __init__(self, model_name, lora_config, output_dim=1):
        super().__init__()
        backbone = AutoModel.from_pretrained(model_name, device_map='cpu')
        # backbone.config.use_cache = False
        self.peft_model = get_peft_model(backbone, lora_config)
        self.config = backbone.config
        hidden_size = backbone.config.hidden_size
        self.vector_head = nn.Linear(hidden_size, output_dim)  # è¾“å‡ºç»´åº¦ä¸º 1

    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå¹¶å¤„ç†å¯èƒ½çš„é¢å¤–å‚æ•°"""
        self.peft_model.enable_input_require_grads()
        if gradient_checkpointing_kwargs is not None:
            self.peft_model.gradient_checkpointing_enable(**gradient_checkpointing_kwargs)
        else:
            self.peft_model.gradient_checkpointing_enable()

    def forward(self, input_ids, attention_mask=None, labels=None):
        # if hasattr(self.peft_model, "gradient_checkpointing"):
        #     print(f"âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨ - å½“å‰æ¨¡å¼: {self.peft_model.is_gradient_checkpointing}")
        # else:
        #     print("âŒ æ¢¯åº¦æ£€æŸ¥ç‚¹æœªæ­£ç¡®åˆå§‹åŒ–")
        outputs = self.peft_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        # è·å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€
        last_hidden = outputs.last_hidden_state  # [B, T, H]
        cls_hidden = last_hidden[:, -1, :]       # [B, H]
        logits = self.vector_head(cls_hidden)    # [B, 1]
        logits = torch.sigmoid(logits).squeeze(-1)  # æ·»åŠ  sigmoid å¹¶å‹ç¼©è‡³ [B]

        loss = None
        if labels is not None:
            loss_fct = nn.MSELoss()  # ä½¿ç”¨ MSE æŸå¤±
            loss = loss_fct(logits, labels)  # è®¡ç®— logits å’Œ labels çš„ MSE

        return CausalLMOutput(
            loss=loss,
            logits=logits
        )


class VectorTrainer(SFTTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_losses = []  # å­˜å‚¨æ¯ä¸ªepochçš„test loss
        self.output_dir = self.args.output_dir
        self.best_test_loss = float("inf")  # å½“å‰æœ€ä¼˜çš„ test loss
        self.early_stop_counter = 0  # æ—©åœè®¡æ•°å™¨
        self.early_stop_patience = 4  # å¦‚æœtest lossè¶…è¿‡Nä¸ªepochæ²¡æœ‰ä¸‹é™ï¼Œå°±early stop

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels", None)
        outputs = model(**inputs, labels=labels)
        loss = outputs.loss
        return (loss, outputs) if return_outputs else loss

    def save_model(self, output_dir=None, _internal_call=False, epoch_tag=None):
        """ä¿å­˜ LoRA å’Œ vector headã€‚
        - epoch_tag: ç”¨äºæ–‡ä»¶å‘½åï¼Œæ”¯æŒ 'epoch{int}' æˆ– 'epoch_best'
        """
        if not self.is_world_process_zero():  # ç¡®ä¿ä»…ä¸»è¿›ç¨‹ä¿å­˜
            return
        output_dir = output_dir or self.output_dir
        os.makedirs(output_dir, exist_ok=True)

        # è®¾ç½® epoch æ ‡ç­¾
        if epoch_tag is None:
            if hasattr(self.state, "epoch") and self.state.epoch is not None:
                epoch_tag = f"epoch{int(self.state.epoch)}"
            else:
                epoch_tag = "epoch_unknown"

        # ä¿å­˜LoRA
        lora_output_dir = os.path.join(output_dir, f"lora_{epoch_tag}")
        self.model.peft_model.save_pretrained(lora_output_dir)

        # ä¿å­˜vector head
        vector_head_path = os.path.join(output_dir, f"vector_head_{epoch_tag}.pt")
        torch.save(self.model.vector_head.state_dict(), vector_head_path)

        print(f"âœ… Saved LoRA and vector head for {epoch_tag} to {output_dir}")

    def _maybe_log_save_evaluate(self, *args, **kwargs):
        """é‡å†™ä»¥åœ¨æ¯ä¸ªepochç»“æŸæ—¶è®°å½•test lossï¼Œå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹"""
        result = super()._maybe_log_save_evaluate(*args, **kwargs)

        if self.state.epoch is not None and "eval_loss" in self.state.log_history[-1]:
            eval_loss = self.state.log_history[-1]["eval_loss"]
            current_epoch = int(self.state.epoch)

            # === æ–°å¢ï¼šåŒæ­¥å„è¿›ç¨‹çš„eval_loss ===
            eval_loss_tensor = torch.tensor(eval_loss).to(self.args.device)
            if torch.distributed.is_initialized():
                torch.distributed.all_reduce(eval_loss_tensor, op=torch.distributed.ReduceOp.AVG)
            avg_eval_loss = eval_loss_tensor.item()

            # è®°å½•å½“å‰test loss
            if self.is_world_process_zero():
                self.test_losses.append({
                    "epoch": current_epoch,
                    "test_loss": eval_loss
                })

                # å¢é‡å†™å…¥æ–‡ä»¶
                loss_file = os.path.join(self.output_dir, "test_losses.jsonl")
                with open(loss_file, "a") as f:
                    f.write(json.dumps({
                        "epoch": current_epoch,
                        "test_loss": eval_loss
                    }) + "\n")

                # ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹
                if eval_loss < self.best_test_loss:
                    self.best_test_loss = eval_loss
                    self.early_stop_counter = 0  # é‡ç½®è®¡æ•°å™¨
                    best_output_dir = os.path.join(self.output_dir, "best-checkpoint")
                    os.makedirs(best_output_dir, exist_ok=True)
                    self.save_model(output_dir=best_output_dir, epoch_tag="epoch_best")
                    print(f"ğŸŒŸ New best model at epoch {current_epoch} with test_loss = {eval_loss:.4f}")
                else:
                    self.early_stop_counter += 1
                    print(f"âš ï¸ No improvement in test loss for {self.early_stop_counter} epoch(s)")

            # === æ–°å¢ï¼šåˆ†å¸ƒå¼åŒæ­¥early_stop_counter ===
            if torch.distributed.is_initialized():
                # å°†è®¡æ•°å™¨è½¬æ¢ä¸ºtensorè¿›è¡ŒåŒæ­¥
                counter_tensor = torch.tensor([self.early_stop_counter]).to(self.args.device)
                torch.distributed.broadcast(counter_tensor, 0)
                self.early_stop_counter = counter_tensor.item()
            
            # æ‰€æœ‰è¿›ç¨‹æ£€æŸ¥æ—©åœæ¡ä»¶
            if self.early_stop_counter >= self.early_stop_patience:
                if self.is_world_process_zero():
                    print(f"â›” Early stopping triggered at epoch {current_epoch}")
                self.control.should_training_stop = True  # æ ¸å¿ƒä¿®æ”¹ï¼šä»…è®¾ç½®åœæ­¢æ ‡å¿—

        return result
    

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", default="/mnt/bn/motor-nlp-team/models/LLM/base_models/Qwen2.5-7B-Instruct", type=str)
    parser.add_argument("--train_data_path", default="", type=str)
    parser.add_argument("--test_data_path", default="", type=str)
    parser.add_argument("--train_consis_path", default="nothing", type=str)
    parser.add_argument("--test_consis_path", default="nothing", type=str)
    parser.add_argument("--train_acc_path", default="", type=str)
    parser.add_argument("--test_acc_path", default="", type=str)
    parser.add_argument("--output_path", default="./train/save_model")
    parser.add_argument("--logger_path", default="./train/save_model")
    parser.add_argument("--per_device_train_batch_size", default=16, type=int)
    parser.add_argument("--per_device_eval_batch_size", default=128, type=int)
    parser.add_argument("--gradient_accumulation_steps", default=8, type=int)
    parser.add_argument("--num_train_epochs", default=10, type=int)
    parser.add_argument("--seed", default=42, type=int)
    parser.add_argument("--weight_decay", default=0.0, type=float)
    parser.add_argument("--r", default=8, type=int)
    parser.add_argument("--alpha", default=16, type=int)
    parser.add_argument("--lora_dropout", default=0.0, type=float)
    parser.add_argument("--train_data_count", default=0, type=int)
    parser.add_argument("--resume_from", action="store_true", help="Whether to resume from saved LoRA + vector head")
    parser.add_argument("--lora_path", type=str, default="", help="Path to saved LoRA")
    parser.add_argument("--vector_head_path", type=str, default="", help="Path to saved vector head")
    parser.add_argument("--dataset", type=str, default="nq", help="Dataset to process")
    parser.add_argument('--shuffle', action='store_true', help='Enable shuffling')

    args = parser.parse_args()

    return args

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True  # å›ºå®š CUDA å·ç§¯ç®—æ³•
    torch.backends.cudnn.benchmark = False    # å…³é—­è‡ªåŠ¨ä¼˜åŒ–

def print_trainable_params(model):
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"Trainable parameters: {trainable}/{total} ({100*trainable/total:.2f}%)")

def main():
    args = get_args()
    set_seed(args.seed)
    accelerator = Accelerator()
    accelerate_set_seed(args.seed) # è®¾ç½®éšæœºç§å­ï¼ˆä½¿ç”¨accelerateçš„ç‰ˆæœ¬ä»¥ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥ï¼‰
    
    if accelerator.is_main_process:
        print(args)
        output_path = args.output_path
        # check output_path
        if not os.path.exists(output_path):
            os.makedirs(output_path)
            print(f"Folder {output_path} Created")
        else:
            print(f"Folder {output_path} Existed")

            # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„test_losses.jsonlæ–‡ä»¶
        loss_file = os.path.join(output_path, "test_losses.jsonl")
        if os.path.exists(loss_file):
            print(f"Warning: {loss_file} already exists, will append new data")
        else:
            with open(loss_file, "w") as f:
                pass  # åˆ›å»ºç©ºæ–‡ä»¶
    accelerator.wait_for_everyone()
    ### Load tokenizer + dataset
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    all_datasets = args.dataset.split('-')
    train_data, train_labels = [], []
    test_data, test_labels = [], []

    if len(all_datasets) == 1:
        if args.train_consis_path != "nothing":
            train_data, train_labels = construct_greedy_answer_conf(args.train_data_path, args.train_consis_path)
            test_data, test_labels = construct_greedy_answer_conf(args.test_data_path, args.test_consis_path)
        else:
            train_data, train_labels = construct_right_answer_conf(args.train_data_path)
            test_data, test_labels = construct_right_answer_conf(args.test_data_path)
    else:
        for dataset in all_datasets:
            print(f'construct data for {dataset}')
            # ä¸ºæ¯ä¸ªæ•°æ®é›†æ„é€ è¾“å…¥æ•°æ®è·¯å¾„
            train_data_path = args.train_data_path.replace(args.dataset, dataset)
            test_data_path = args.test_data_path.replace(args.dataset, dataset)
            train_consis_path = args.train_consis_path.replace(args.dataset, dataset)
            test_consis_path = args.test_consis_path.replace(args.dataset, dataset)

            if args.train_consis_path != "nothing":
                tmp_train_data, tmp_train_labels = construct_greedy_answer_conf(train_data_path, train_consis_path)
                tmp_test_data, tmp_test_labels = construct_greedy_answer_conf(test_data_path, test_consis_path)
            else:
                tmp_train_data, tmp_train_labels = construct_right_answer_conf(train_data_path)
                tmp_test_data, tmp_test_labels = construct_right_answer_conf(test_data_path)

            train_data.extend(tmp_train_data)
            train_labels.extend(tmp_train_labels)
            test_data.extend(tmp_test_data)
            test_labels.extend(tmp_test_labels)

        # shuffle train_data and train_labels
        combined = list(zip(train_data, train_labels))
        if args.shuffle:
            print('shuffle train data')
            random.shuffle(combined)
        train_data, train_labels = zip(*combined)
        train_data, train_labels = list(train_data), list(train_labels)
            
    train_data_cnt = len(train_data) if args.train_data_count == 0 else args.train_data_count
    if accelerator.is_main_process:
        print(f'train data count: {train_data_cnt}')

    if args.train_data_count == 0:
        # å¦‚æœ train_data_count ä¸º 0ï¼Œå–å…¨éƒ¨æ•°æ®
        train_dataset = Dataset.from_dict({
            "text": train_data,
            "label": train_labels
        })
    else:
        # ç”Ÿæˆéšæœºç´¢å¼•
        random_indices = random.sample(range(len(train_data)), train_data_cnt)
        # æ ¹æ®éšæœºç´¢å¼•é€‰æ‹©æ•°æ®
        sampled_train_data = [train_data[i] for i in random_indices]
        sampled_train_labels = [train_labels[i] for i in random_indices]
        
        train_dataset = Dataset.from_dict({
            "text": sampled_train_data,
            "label": sampled_train_labels
        })

    test_dataset = Dataset.from_dict({
    "text": test_data,
    "label": test_labels
    })
    collator = DataCollatorWithVectorLabel(tokenizer)
    
    ### 5. Training arguments
    # çœ‹äº†ä»£ç å‘ç°, SFTTraineré‡Œé¢çš„é»˜è®¤æ“ä½œæ˜¯åšcasual language modelingä»»åŠ¡è€Œä¸æ˜¯SFT, æ¯”å¦‚é»˜è®¤çš„collatorä¸ä¼šæŠŠpromptéƒ¨åˆ†çš„labelsè®¾ç½®ä¸º-100
    # å¿½ç•¥promptéƒ¨åˆ†çš„losséœ€è¦ç”¨DataCollatorForCompletionOnlyLM
    training_args = TrainingArguments(
        output_dir=args.output_path,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_ratio=0.1,
        num_train_epochs=args.num_train_epochs,
        report_to="tensorboard",
        logging_steps=1,
        logging_dir=args.logger_path,
        remove_unused_columns=False,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        seed=args.seed,
        eval_strategy="epoch",
        save_strategy="no",
        optim="adamw_torch",
        weight_decay=args.weight_decay,
        gradient_checkpointing=False,
        ddp_find_unused_parameters=False,  # é‡è¦DDPå‚æ•°
        fsdp="",  # æ˜ç¡®ä¸ä½¿ç”¨FSDP
    )
    if accelerator.is_main_process:
        print(training_args)

    ### 6. Model and trainer
    lora_config = LoraConfig(
        r=args.r,
        lora_alpha=args.alpha,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                        "gate_proj", "up_proj", "down_proj"],
        lora_dropout=args.lora_dropout,
        bias="none",
    )
    model = LMWithVectorHead(args.model_path, lora_config)

    if args.resume_from:
        base_model = AutoModel.from_pretrained(args.model_path, device_map='cpu')
        peft_model = PeftModel.from_pretrained(
            base_model,
            args.lora_path,
            adapter_name="default",
            device_map='cpu'
        )
        model.peft_model = peft_model

        state_dict = torch.load(args.vector_head_path, map_location="cpu")
        model.vector_head.load_state_dict(state_dict)
        model.peft_model.set_adapter("default")

    if accelerator.is_main_process:
        print(f'use cache: {model.config.use_cache}')  # åº”è¯¥è¾“å‡ºFalse
        print_trainable_params(model)

    trainer = VectorTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=collator
    )

    ### 7. Train
    trainer.train()
    # ä¿å­˜ LoRA å‚æ•°
    # model.peft_model.save_pretrained(output_path)  # ä»… LoRA

    # # ä¿å­˜ vector_head å‚æ•°
    # torch.save(model.vector_head.state_dict(), os.path.join(output_path, "vector_head.pt"))

if __name__ == "__main__":
    main()